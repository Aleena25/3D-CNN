import pickle

import tensorflow
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Model
import numpy as np
from tensorflow.keras import utils
from sklearn.preprocessing import LabelEncoder

x1train = pickle.load( open( r" ", "rb" ) )
x2train = pickle.load( open( r" ", "rb" ) )
x3train = pickle.load( open( r" ", "rb" ) )
x4train = pickle.load( open( r" ", "rb" ) )

x1test = pickle.load( open( r" ", "rb" ) )
x2test = pickle.load( open( r" ", "rb" ) )
x3test = pickle.load( open( r" ", "rb" ) )
x4test = pickle.load( open( r" ", "rb" ) )


ytrain = pickle.load( open( r" ", "rb" ) )
ytest = pickle.load( open( r" ", "rb" ) )
###############################################################################
encoder = LabelEncoder()
encoder.fit(ytrain)
encoded_Y1 = encoder.transform(ytrain)
ytrain = utils.to_categorical(encoded_Y1)

encoder.fit(ytest)
encoded_Y2 = encoder.transform(ytest)
ytest = utils.to_categorical(encoded_Y2)

###############################################################################
def conv1(input_layer):
    x = layers.Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(1,1,1), padding='same', activation="relu")(input_layer)
    x = layers.MaxPool3D((3, 3, 3), strides=(2, 2, 2), padding="same")(x)
    x = layers.BatchNormalization()(x)
    return x
def conv2(x):    
    x = layers.Conv3D(filters=64, kernel_size=(3,3,3), strides=(2,2,1), padding='same', activation="relu")(x)
    # x = layers.MaxPool3D(pool_size=2)(x)
    x = layers.BatchNormalization()(x)
    return x
def conv3(x):
    def f(x):
        x = layers.Conv3D(filters=64, kernel_size=(3,3,3),strides=(1,1,1), padding='same',activation="relu")(x)
        # x = layers.MaxPool3D(pool_size=2)(x)
        x = layers.BatchNormalization()(x)
        return f
    return x
def _shortcut(x, conv3):
    """Adds a shortcut between input and residual block and merges them with "sum"
    """

    shortcut = layers.Conv3D(filters=64,
                          kernel_size=(3,3,3),
                          strides=(2,2,1), padding='same', activation="relu" )(x)
    return layers.add([shortcut, conv3])
def concat_layer(x1, x2, x3, x4):
    concatted = keras.layers.concatenate([x1, x2, x3, x4])
    return concatted
def conv4(x):
    x = layers.Conv3D(filters=128, kernel_size=(3,3,3), strides=(2,2,2), padding='same')(x)
    x = layers.AveragePooling3D(pool_size=(3,3,3), strides=(2,2,2), padding='same')(x)
    x = layers.BatchNormalization()(x)
    return x
def output_layers(x):
    x = layers.Flatten()(x)
    x = layers.Dense(units=256, activation="relu")(x)
    x = layers.Dropout(0.3)(x)
    x = layers.Dense(units=6, activation="softmax")(x)
    return x

###############################################################################
inputs1 = keras.Input((3, 100, 32,1))
x11 = conv1(inputs1)
x1 = conv2(x11)
x1 = _shortcut(x11, conv3(x1))

inputs2 = keras.Input((1, 100, 32,1))
x22 = conv1(inputs2)
x2 = conv2(x22)
x2 = _shortcut(x22, conv3(x2))

inputs3 = keras.Input((1, 100, 32,1))
x33 = conv1(inputs3)
x3 = conv2(x33)
x3 = _shortcut(x33, conv3(x3))

inputs4 = keras.Input((1, 100, 32,1))
x44 = conv1(inputs4)
x4 = conv2(x44)
x4 = _shortcut(x44, conv3(x4))


merged = concat_layer(x1, x2, x3, x4)
x = conv4(merged)
output = output_layers(x)
model = Model(inputs=[inputs1, inputs2, inputs3, inputs4], outputs=output, name="pointcnn")

# compile
# initial_learning_rate = 0.001
# lr_schedule = tensorflow.keras.optimizers.schedules.ExponentialDecay(
#     initial_learning_rate, decay_steps=100000, decay_rate=0.0, staircase=True
# )
# model.compile(loss='categorical_crossentropy', optimizer=tensorflow.keras.optimizers.Adam(learning_rate=lr_schedule), metrics=['acc'])
# summarize
model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])
print(model.summary())

#history = model.fit(x=[x1train, x2train, x3train, x4train], y=np.array(ytrain), batch_size=16, epochs=50, validation_split=0.2)
history = model.fit(x=[x1train, x2train, x3train, x4train], y=ytrain, batch_size=50, epochs=50, validation_split=0.2)

#model.save("my_model")
model_json = model.to_json()
with open("model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("model.h5")
print("Saved model to disk")
# evaluate model on training dataset
loss, acc = model.evaluate([x1train, x2train, x3train, x4train], ytrain, verbose=0)
print('Train Accuracy: %f' % (acc*100))
 
# evaluate model on test dataset dataset
loss, acc = model.evaluate([x1test, x2test, x3test, x4test], ytest, verbose=0)
print('Test Accuracy: %f' % (acc*100))

    
